import os
import subprocess
from openai import OpenAI


url_txt = "urls.txt"           # åŒ…å« YouTube é“¾æ¥çš„ txt æ–‡ä»¶
save_dir = "downloads"         # ä¸‹è½½ä¸è¾“å‡ºç›®å½•
api_key = ""  # æ›¿æ¢ä¸ºä½ çš„ OpenAI API Key
base_url = "https://openrouter.ai/api/v1"
max_tokens = 5000


def download_video_and_subtitles(url, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    command = [
        "python", "-m", "yt_dlp",
        "--write-auto-sub", "--sub-lang", "en",
        "--skip-download",
        "-o", os.path.join(output_dir, "%(title)s.%(ext)s"),
        url
    ]
    subprocess.run(command, shell=True)


def extract_unique_lines_from_vtt(vtt_path):
    output_lines = []
    previous_line = ""

    with open(vtt_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    for i in range(len(lines)):
        line = lines[i].strip()
        if "start position:0%" in line and i + 1 < len(lines):
            content = lines[i + 1].strip()
            if content and content != previous_line:
                output_lines.append(content)
                previous_line = content

    return ' '.join(output_lines)


def process_all_urls(url_file, output_dir):
    with open(url_file, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]

    for url in urls:
        print(f"\nğŸ¬ æ­£åœ¨å¤„ç†: {url}")
        download_video_and_subtitles(url, output_dir)

    processed_txt_files = []

    for file in os.listdir(output_dir):
        if file.endswith(".vtt"):
            vtt_path = os.path.join(output_dir, file)
            clean_text = extract_unique_lines_from_vtt(vtt_path)

            txt_path = vtt_path.replace(".vtt", ".txt")
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(clean_text)
            print(f"âœ… å·²ç”Ÿæˆå­—å¹•æ–‡æœ¬: {txt_path}")

            processed_txt_files.append(txt_path)

    return processed_txt_files


def summarize_txts_with_context(txt_paths):
    """
    æ‰¹é‡å¯¹å¤šä¸ªå­—å¹• txt æ–‡ä»¶è¿›è¡Œä¸Šä¸‹æ–‡æ€»ç»“ï¼Œè¾“å‡ºä¸º *_summary.txt æ–‡ä»¶ã€‚
    """
    client = OpenAI(api_key=api_key, base_url=base_url,
                    default_headers={  # å…³é”®ä¿®æ”¹ç‚¹
                        "HTTP-Referer": "http://localhost",  # æœ¬åœ°å¼€å‘ä¸“ç”¨æ ‡è¯†
                        "X-Title": "Youtube Summary Tool"    # åº”ç”¨åç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
                    })

    for txt_path in txt_paths:
        with open(txt_path, "r", encoding="utf-8") as f:
            text = f.read()

        # åˆ†æ®µæ–‡æœ¬
        chunk_size = 3000
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­—å¹•æ€»ç»“ä¸“å®¶ï¼Œå¸®åŠ©ç”¨æˆ·å‡†ç¡®ç†è§£è§†é¢‘å†…å®¹ï¼Œç‰¹åˆ«å…³æ³¨å„ç§è½¯ä»¶å’Œå¹³å°çš„ä½¿ç”¨æµç¨‹ã€‚"}
        ]

        for i, chunk in enumerate(chunks):
            messages.append({
                "role": "user",
                "content": f"è¿™æ˜¯è§†é¢‘å­—å¹•çš„ç¬¬ {i+1} æ®µï¼Œè¯·é˜…è¯»å¹¶è®°ä½å†…å®¹ï¼Œç¨åæˆ‘ä¼šè®©ä½ æ€»ç»“ï¼š\n{chunk}"
            })

        messages.append({
            "role": "user",
            "content": "ç°åœ¨è¯·ä½ å¯¹å‰é¢æ‰€æœ‰æä¾›çš„å­—å¹•å†…å®¹è¿›è¡Œæ•´ä½“æ€»ç»“ã€‚è¯·ç»†è‡´è¯´æ˜å­—å¹•ä¸­æåˆ°çš„è½¯ä»¶å’Œå¹³å°æ˜¯å¦‚ä½•è¢«ä½¿ç”¨çš„ï¼Œå°¤å…¶è¦ä¿ç•™æ“ä½œæ­¥éª¤å’Œå…³é”®ç»†èŠ‚ã€‚"
        })

        print(f"ğŸ§  æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡æ€»ç»“ï¼š{txt_path}")
        response = client.chat.completions.create(
            model="deepseek/deepseek-r1-zero:free",
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.5
        )

        summary = response.choices[0].message.content

        output_path = txt_path.replace(".txt", "_summary.txt")
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(summary)

        print(f"âœ… æ€»ç»“å®Œæˆï¼Œä¿å­˜äºï¼š{output_path}")


if __name__ == "__main__":

    # Step 1: ä¸‹è½½è§†é¢‘å­—å¹• + æå– txt
    txt_files = process_all_urls(url_txt, save_dir)

    # Step 2: ä½¿ç”¨ OpenAI API æ€»ç»“
    summarize_txts_with_context(txt_files)
